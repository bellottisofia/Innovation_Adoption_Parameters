# -*- coding: utf-8 -*-
"""NL_feature_selection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o2nipnx_0di74wacNCs6g5KcaZK-wY2F

## Importing the Libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import warnings
from sklearn.exceptions import ConvergenceWarning

# Ignore FutureWarnings
warnings.filterwarnings("ignore", category=FutureWarning)
# Ignore ConvergenceWarnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

pd.set_option('future.no_silent_downcasting', True)

"""## Importing the dataset"""

df = pd.read_csv('/content/(3)_NL_EVsurvey_data_EN_datavalues.csv')
#df1.replace('?', np.nan, inplace=True)

#df2 = pd.read_csv('/content/(4)_NL_EVsurvey_data_EN_datalabels.csv')

#df = pd.concat([df2, df1], axis=1)

df.head(5)

list(df.columns)

df.drop(columns=["xDLNMID","xtEIND", "xtConditi",'nRegion','xAge',
 'nAgeCat',
 'nGender',
 'xHighLevEdu',
 'nEduLev',
 'xIncome',
 'nIncCat',
 'nProv',
 'nRegion',
 'nShoopResp',
 'wwighfact',
 'nGen',
 'nAg',
 'nEdu',
 'nReg','mQ13_16',
 'mQ13_99999997','mQ12_99999997','mQ11_99999997'], inplace=True)

"""## Data Preprocessing

### Data Analysis
"""

df.describe()

df.info()

'''
# Define the bins and labels
bins = [0, 25, 50, 75, float("inf")]  # Define bin edges
labels = ["Under 25", "25-50", "50-75", "Over 75"]  # Define category labels

# Assign labels to each age
df["Age_Cat"] = pd.cut(df["xAge"], bins=bins, labels=labels, right=True)
'''

'''
# Custom encoding mappings
encoding_maps = {

    'Age_Cat': {
        'Under 25': 0,
        '25-50': 1,
        '50-75': 2,
        'Over 75': 3
    }
}

# Apply the custom encoding
for col, mapping in encoding_maps.items():
    df[col] = df[col].map(mapping)

# Remove rows with NaN (if there are unmapped values)
df = df.dropna()

# Save the encoded dataset
df.to_csv("custom_encoded_dataset.csv", index=False)

print("Custom label encoding completed and saved as 'custom_encoded_dataset.csv'!")'''

df["nGen"].value_counts()

df["nEdu"].value_counts()

df["nQ16"].value_counts()

df["nQ16"].value_counts().plot.pie(autopct='%1.1f%%')

plt.title("Distribution of Q16 answers")
plt.axis('equal')
plt.show()

"""### Transforming the dataset for Binary Classifaciton

A and B labels as 1. C and D labels as 0
"""

# Define a mapping dictionary
mapping = {1: 1, 2: 1, 3: 0, 99999997: 0}

# Apply the mapping to the target variable
df['nQ16'] = df['nQ16'].map(mapping)

#df.drop(columns=["nAgeCat","xAge",'nIncCat','nAg'], inplace=True)

df.head(10)

df["nQ16"].value_counts()

df["nQ16"].value_counts().plot.pie(autopct='%1.1f%%')

plt.title("Distribution of Q16 answers in Binary Format")
plt.axis('equal')
plt.show()

"""### Taking care of missing data"""

df.fillna(df.mode().iloc[0], inplace=True) #replacing the nan values with the mode of each colmun

"""### Balancing the imbalanced data"""

from imblearn.over_sampling import SMOTE

random_over_sampler = SMOTE(random_state=42)

y = df['nQ16']
X = df.drop(['nQ16'],axis=1)

# Get the column names of the features
feature_names = X.columns

X_resampled , y_resampled = random_over_sampler.fit_resample(X,y)
#generating data
balanced_df = pd.DataFrame(X_resampled, columns=X.columns)
balanced_df['nQ16'] = y_resampled

balanced_df["nQ16"].value_counts()

balanced_df["nQ16"].value_counts().plot.pie(autopct='%1.1f%%')

plt.title("Distribution of nQ16 answers in Binary Format")
plt.axis('equal')
plt.show()

"""## Creating X(independent) and Y(dependent)"""

y = balanced_df['nQ16'].values
X = balanced_df.drop(['nQ16'],axis=1).values

"""## Spliting the dataset into the training set and test set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

print(X_train.shape)
print(X_test.shape)

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print(X_train)



"""## Baseline Function To perform features selection

"""

def combined_feature_selection_trainset(X_train, y_train, feature_names,top_k=10):
    import numpy as np
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, RFE
    from sklearn.preprocessing import StandardScaler, MinMaxScaler


    feature_votes = pd.Series(0, index=feature_names)

    scaler_std = StandardScaler()
    X_scaled = scaler_std.fit_transform(X_train)

    scaler_mm = MinMaxScaler()
    X_mm = scaler_mm.fit_transform(X_train)

    # 1. Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    rf_top_idx = np.argsort(rf.feature_importances_)[-top_k:]
    feature_votes.iloc[rf_top_idx] += 1

    # 2. Logistic Regression (L1)
    logreg = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42)
    logreg.fit(X_scaled, y_train)
    l1_top_idx = np.argsort(np.abs(logreg.coef_)[0])[-top_k:]
    feature_votes.iloc[l1_top_idx] += 1

    # 3. Chi²
    chi2_selector = SelectKBest(score_func=chi2, k=top_k)
    chi2_selector.fit(X_mm, y_train)
    chi2_idx = chi2_selector.get_support(indices=True)
    feature_votes.iloc[chi2_idx] += 1

    # 4. RFE
    rfe = RFE(estimator=LogisticRegression(solver='liblinear'), n_features_to_select=top_k)
    rfe.fit(X_scaled, y_train)
    rfe_idx = rfe.get_support(indices=True)
    feature_votes.iloc[rfe_idx] += 1

    # 5. Mutual Information
    mi_scores = mutual_info_classif(X_train, y_train, random_state=42)
    mi_top_idx = np.argsort(mi_scores)[-top_k:]
    feature_votes.iloc[mi_top_idx] += 1

    # Ordina per numero di voti
    feature_votes = feature_votes.sort_values(ascending=False)
    selected_features = feature_votes[feature_votes > 0].index.tolist()

    print("\n Feature selezionate più frequentemente:")
    print(feature_votes[feature_votes > 0])

    return selected_features

def train_and_test_selected_features_df(classifier, X_train, X_test, y_train, y_test, selected_features, feature_names):
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score

    # Seleziona gli indici delle colonne che corrispondono ai nomi in selected_features
    feature_indices = [i for i, col in enumerate(feature_names) if col in selected_features]

    # Seleziona solo le colonne scelte usando gli indici
    X_train_sel = X_train[:, feature_indices]  # Usa l'indicizzazione numerica con numpy
    X_test_sel = X_test[:, feature_indices]    # Usa l'indicizzazione numerica con numpy

    # Standardizzazione dei dati
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_sel)
    X_test_scaled = scaler.transform(X_test_sel)

    # Addestra il classificatore
    classifier.fit(X_train_scaled, y_train)
    y_pred = classifier.predict(X_test_scaled)

    # Calcola l'accuracy
    acc = accuracy_score(y_test, y_pred)
    print(f"\n Accuracy usando le feature selezionate: {acc:.4f}")
    return acc

# Se hai già X_train, X_test, y_train, y_test come DataFrame/Series
selected = combined_feature_selection_trainset(X_train, y_train, feature_names=feature_names, top_k=20)

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(max_iter=1000)

train_and_test_selected_features_df(clf, X_train, X_test, y_train, y_test, selected,feature_names=feature_names)